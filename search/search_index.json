{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Reddit Forecast Documentation","text":"<p>Welcome to the Reddit Forecast project documentation. This site provides comprehensive information about the project's API, data processing, and model details.</p>"},{"location":"#table-of-contents","title":"\ud83d\udcda Table of Contents","text":"<ul> <li>API Reference</li> <li>Data</li> <li>Train</li> <li>Model</li> <li>How to run</li> </ul>"},{"location":"#about-the-project","title":"\ud83d\udee0\ufe0f About the Project","text":"<p>Reddit Forecast is a tool designed to analyze Reddit posts, perform sentiment analysis, and provide insights into stock discussions and sentiments.</p>"},{"location":"#welcome-to-reddit-forecast-documentation_1","title":"Welcome to Reddit Forecast Documentation","text":"<p>Welcome to the Reddit Forecast project documentation. This site provides comprehensive information about the project's API, data processing, and model details.</p>"},{"location":"#table-of-contents_1","title":"\ud83d\udcda Table of Contents","text":"<ul> <li>API Reference</li> <li>Data</li> <li>Train</li> <li>Model</li> <li>How to run</li> </ul>"},{"location":"#about-the-project_1","title":"\ud83d\udee0\ufe0f About the Project","text":"<p>Reddit Forecast is a tool designed to analyze Reddit posts, perform sentiment analysis, and provide insights into stock discussions and sentiments.</p>"},{"location":"#people-who-helped","title":"\ud83e\udd1d People Who Helped","text":"<p>This project is a collaborative effort. Here are the people who were a part of the project</p> <ul> <li>[Martin S. Jespersen]</li> <li>[Lucas Sylvester]</li> <li>[Lucas 2]</li> <li>[Marcus Elkj\u00e6r]</li> <li>[Sebastian Wulf Andersen]</li> </ul>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>Sentiment Analysis: Analyze the sentiment of Reddit discussions related to stock tickers.</li> <li>Data Visualization: Generate insightful plots for class distribution and other metrics.</li> <li>Custom API: Query sentiment scores and retrieve processed data programmatically.</li> <li>Scalable Model Training: Leverage preprocessed data to train machine learning models.</li> </ul>"},{"location":"#project-structure","title":"\ud83d\udcc2 Project Structure","text":"<pre><code>\u251c\u2500\u2500 .github/                  # Github actions and dependabot\n\u2502   \u251c\u2500\u2500 dependabot.yaml\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u2514\u2500\u2500 tests.yaml\n\u251c\u2500\u2500 configs/                  # Configuration files\n\u251c\u2500\u2500 data/                     # Data directory\n\u2502   \u251c\u2500\u2500 processed\n\u2502   \u2514\u2500\u2500 raw\n\u251c\u2500\u2500 dockerfiles/              # Dockerfiles\n\u2502   \u251c\u2500\u2500 api.Dockerfile\n\u2502   \u2514\u2500\u2500 train.Dockerfile\n\u251c\u2500\u2500 docs/                     # Documentation\n\u2502   \u251c\u2500\u2500 mkdocs.yml\n\u2502   \u2514\u2500\u2500 source/\n|       \u251c\u2500\u2500 otherMDFiles.md\n\u2502       \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 models/                   # Trained models\n\u251c\u2500\u2500 notebooks/                # Jupyter notebooks\n\u251c\u2500\u2500 reports/                  # Reports\n\u2502   \u2514\u2500\u2500 figures/\n\u251c\u2500\u2500 src/                      # Source code\n\u2502   \u251c\u2500\u2500 project_name/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u251c\u2500\u2500 data.py\n\u2502   \u2502   \u251c\u2500\u2500 evaluate.py\n\u2502   \u2502   \u251c\u2500\u2500 models.py\n\u2502   \u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2502   \u2514\u2500\u2500 visualize.py\n\u2514\u2500\u2500 tests/                    # Tests\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_api.py\n\u2502   \u251c\u2500\u2500 test_data.py\n\u2502   \u251c\u2500\u2500 test_model.py\n|   \u2514\u2500\u2500 test_train.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .pre-commit-config.yaml\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 pyproject.toml            # Python project file\n\u251c\u2500\u2500 README.md                 # Project README\n\u251c\u2500\u2500 requirements.txt          # Project requirements\n\u251c\u2500\u2500 requirements_dev.txt      # Development requirements\n\u2514\u2500\u2500 tasks.py                  # Project tasks\n</code></pre>"},{"location":"data/","title":"Data Module Overview","text":"<p>This module handles data loading, preprocessing, and statistics generation for Reddit ticker data. It is a critical part of the pipeline, ensuring that raw data is transformed into a usable format for analysis and machine learning tasks.</p>"},{"location":"data/#key-functions","title":"\u26a1 Key Functions","text":""},{"location":"data/#preprocessraw_data_path-output_folder","title":"preprocess(raw_data_path, output_folder)","text":"<ul> <li>Description: Reads the JSON file containing raw data, processes it, and outputs cleaned data as CSV files.</li> <li>Inputs:</li> <li><code>raw_data_path (Path)</code>: Path to the folder containing the raw JSON file (<code>saps.json</code>).</li> <li><code>output_folder (Path)</code>: Path to save the processed data.</li> <li>Outputs:</li> <li><code>preprocessed_data.csv</code>: Cleaned and preprocessed data.</li> <li>Logging:</li> <li>Logs the data processing steps.</li> <li>Raises:</li> <li><code>FileNotFoundError</code> if the JSON file does not exist.</li> </ul>"},{"location":"data/#dataset_statisticsprocessed_path","title":"dataset_statistics(processed_path)","text":"<ul> <li>Description: Analyzes the preprocessed data to compute key statistics and save a class distribution plot.</li> <li>Inputs:</li> <li><code>processed_path (Path)</code>: Path to the folder containing <code>preprocessed_data.csv</code>.</li> <li>Outputs:</li> <li><code>statistics_report.md</code>: A Markdown file with dataset statistics.</li> <li><code>class_distribution.png</code>: A bar plot of class distribution.</li> <li>Raises:</li> <li><code>FileNotFoundError</code> if the <code>preprocessed_data.csv</code> file does not exist.</li> </ul>"},{"location":"data/#mydataset-class","title":"MyDataset (class)","text":"<ul> <li>Description: A PyTorch <code>Dataset</code> class for loading and accessing preprocessed data.</li> <li>Attributes:</li> <li><code>csv_path (Path)</code>: Path to the CSV file.</li> <li><code>transform</code>: Optional transformation applied to the data rows.</li> <li>Methods:</li> <li><code>__len__</code>: Returns the number of rows in the dataset.</li> <li><code>__getitem__(idx)</code>: Retrieves a row by index and applies optional transformations.</li> </ul>"},{"location":"data/#file-structure","title":"\ud83d\udcc2 File Structure","text":"<pre><code>project/\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u2502   \u2514\u2500\u2500 saps.json\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2502   \u251c\u2500\u2500 preprocessed_data.csv\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 data.py\n</code></pre>"},{"location":"data/#usage","title":"\ud83d\udee0\ufe0f Usage","text":""},{"location":"data/#preprocessing-data","title":"Preprocessing Data","text":"<ol> <li>Place the raw data file (<code>saps.json</code>) in the <code>data/raw/</code> folder or have them downloaded with kaggle/our bucket</li> <li>Run the preprocessing function:</li> </ol> <pre><code>python src/data.py\n</code></pre> <p>This will: - Load raw JSON data. - Clean and preprocess the data. - Save the output to <code>data/processed/preprocessed_data.csv</code>.</p>"},{"location":"data/#generating-dataset-statistics","title":"Generating Dataset Statistics","text":"<ol> <li>Ensure <code>preprocessed_data.csv</code> exists in <code>data/processed/</code>.</li> <li>Call the <code>dataset_statistics</code> function:</li> </ol> <pre><code>from data import dataset_statistics\n\nprocessed_path = Path(\"data/processed\")\ndataset_statistics(processed_path)\n</code></pre> <p>This will: - Compute and save statistics in <code>statistics_report.md</code>.</p>"},{"location":"data/#example-output","title":"\ud83d\udcca Example Output","text":""},{"location":"data/#dataset-statistics-report","title":"Dataset Statistics Report","text":""},{"location":"data/#sample-output-from-statistics_reportmd","title":"Sample Output from <code>statistics_report.md</code>:","text":"<pre><code># Dataset Statistics\n\nNumber of samples: 12345\nNumber of unique tickers: 678\n\n# Dataset Statistics\n\nNumber of samples: 4800\nNumber of unique tickers: 592\n\n## Class Distribution\n\n| flair      |   count |\n|:-----------|--------:|\n| Discussion |     724 |\n| Question   |     355 |\n| Shitpost   |     311 |\n| Research   |     299 |\n| News       |     238 |\n| Positions  |     208 |\n| Rants      |      34 |\n| Options    |       8 |\n</code></pre>"},{"location":"frontend/","title":"Frontend","text":""},{"location":"frontend/#how-to-run","title":"How to run","text":"<p>Uvicorn is used to serve both the frontend and the api It can be run simply with the following cmd, after installing requirements.</p> <pre><code>uvicorn reddit_forecast.api:app --reload --port 8000\n</code></pre>"},{"location":"frontend/#how-to-use","title":"How to use","text":"<p>The frontend is available in the root \"/\".</p> <p>Input into the field any Ticker symbol (letter combination representing a stock). The graphs show the latest months stock price and the average sentiment for the stock each day.</p>"},{"location":"frontend/#datadrift-report","title":"Datadrift report","text":"<p>To acces the data drift report go to /report. The report compares reference data that is pulled at the time of running, with evaluation data for the report. Both datasets are from the r/RobinHoodPennyStocks subreddit. If you wish to regenerate the report run:</p> <pre><code>python -m reddit_forecast.data_drift\n</code></pre>"},{"location":"how_to_run/","title":"How to Run Reddit Forecast","text":""},{"location":"how_to_run/#installation","title":"Installation","text":"<ol> <li>Clone the Repository:</li> </ol> <p><code>bash    git clone https://github.com/reddit-forecast/reddit-forecast.git    cd reddit-forecast</code> 2. Set Up a Virtual Environment (Optional but recommended):</p> <p><code>bash    python -m venv venv    source venv/bin/activate   # On Windows: venv\\Scripts\\activate</code> 3. Install Dependencies:</p> <p><code>bash    pip install -r requirements.txt</code> 4. Pull Data with DVC:</p> <p><code>bash    dvc pull</code></p>"},{"location":"how_to_run/#running-the-project","title":"Running the Project","text":""},{"location":"how_to_run/#1-preprocess-data","title":"1. Preprocess Data","text":"<p>Run the preprocessing script to clean and prepare the data:</p> <p><code>bash    python src/data.py</code>     The preprocessed data will be saved in the <code>data/processed/</code> folder.</p>"},{"location":"how_to_run/#2-run-the-model","title":"2. Run the Model","text":"<p>Run the machine learning model using the processed data:</p> <p><code>bash    python src/model.py</code>    This will generate model files in the <code>models/</code> directory.</p>"},{"location":"how_to_run/#3-train-the-model","title":"3. Train the Model","text":"<p>Train the machine learning model using the processed data:</p> <p><code>bash    python src/train.py</code>    This will generate model files in the <code>models/</code> directory.</p>"},{"location":"how_to_run/#4-run-the-api","title":"4. Run the API","text":"<p>Launch the API to interact with Reddit Forecast programmatically:</p> <p><code>bash    uvicorn reddit_forecast.api:app --reload --port 8000</code></p> <p>The API will be available at <code>http://127.0.0.1:8000</code>.</p>"},{"location":"how_to_run/#5-test-the-project","title":"5. Test the Project","text":"<p>Run unit tests to ensure the project is functioning correctly:</p> <p><code>bash    pytest tests/</code></p>"},{"location":"my_api/","title":"API Reference","text":"<p>This section provides detailed information about the API endpoints available in the Reddit Forecast project.</p>"},{"location":"my_api/#how-to-run","title":"How to run","text":"<p>Uvicorn is used to serve both the frontend and the api It can be run simply with the following cmd, after installing requirements.</p> <pre><code>uvicorn reddit_forecast.api:app --reload --port 8000\n</code></pre>"},{"location":"my_api/#endpoints","title":"Endpoints","text":""},{"location":"my_api/#analyze-sentiment","title":"Analyze Sentiment","text":"<p>Endpoint: <code>/analyze_sentiment</code></p> <p>Method: <code>GET</code></p> <p>Description: Analyzes the sentiment of the provided text.</p> <p>Parameters: - <code>text</code> (str): The text to analyze.</p> <p>Response: - <code>200 OK</code>: Returns the sentiment of the text.</p> <p>Example Request:</p> <pre><code>GET /analyze_sentiment?text=I am happy\n</code></pre> <p>Example Reponse</p> <pre><code>\n</code></pre>"},{"location":"my_api/#get-posts","title":"Get Posts","text":"<p>Endpoint: <code>/get_posts</code></p> <p>Method: <code>GET</code></p> <p>Description: Retrieves posts from a specified subreddit based on a search term and analyzes their sentiment.</p> <p>Parameters: - <code>search_term</code> (str): The term to search for in the subreddit. - <code>subreddit</code> (str, optional): The subreddit to search in. Default is <code>wallstreetbets</code>.</p> <p>Response: - <code>200 OK</code>: Returns a list of posts with their sentiment analysis.</p> <p>Example Request:</p> <pre><code>GET /get_posts?search_term=GameStop&amp;subreddit=wallstreetbets\n</code></pre> <p>Example Response</p> <pre><code>[\n  {\n    \"title\": \"GameStop to the moon!\",\n    \"url\": \"https://www.reddit.com/r/wallstreetbets/comments/...\",\n    \"created_date\": \"2023-10-01\",\n    \"score\": 1234,\n    \"num_comments\": 567,\n    \"selftext\": \"GameStop is going to skyrocket!\",\n    \"sentiment\": \"POSITIVE\"\n  },\n  ...\n]\n</code></pre>"},{"location":"my_api/#get-average-sentiment","title":"Get Average Sentiment","text":"<p>Endpoint: <code>/get_average_sentiment</code></p> <p>Method: <code>GET</code></p> <p>Description: Retrieves posts from a specified subreddit based on a search term and calculates the average sentiment per day.</p> <p>Parameters: - <code>search_term</code> (str): The term to search for in the subreddit. - <code>subreddit</code> (str, optional): The subreddit to search in. Default is <code>wallstreetbets</code>.</p> <p>Response: - <code>200 OK</code>: Returns the average sentiment per day.</p> <p>Example Request:</p> <pre><code>GET /get_average_sentiment?search_term=GameStop&amp;subreddit=wallstreetbets\n</code></pre> <p>Example Response</p> <pre><code>[\n  {\n    \"date\": \"2023-10-01\",\n    \"average_sentiment\": 0.75\n  },\n  ...\n]\n</code></pre>"},{"location":"my_api/#get-report","title":"Get Report","text":"<p>Endpoint: <code>/report</code></p> <p>Method: <code>GET</code></p> <p>Description: Generates and returns the data drift report.</p> <p>Response: - <code>200 OK</code>: Returns the HTML content of the data drift report.</p> <p>Example Request:</p> <pre><code>GET /report\n</code></pre>"},{"location":"train/","title":"Train documentation","text":""},{"location":"train/#this-project-implements-a-sentiment-regression-model-using-distilbert-for-analyzing-and-predicting-sentiment-scores-from-a-dataset-of-labeled-sentences-the-implementation-leverages-pytorch-pytorch-lightning-transformers-and-optuna-for-hyperparameter-tuning-the-data-set-that-we-are-looking-at-is-reddit-penny-stocks-found-on-kaggle","title":"This project implements a sentiment regression model using <code>DistilBERT</code> for analyzing and predicting sentiment scores from a dataset of labeled sentences. The implementation leverages <code>PyTorch</code>, <code>PyTorch Lightning</code>, <code>Transformers</code>, and <code>Optuna</code> for hyperparameter tuning. The data set that we are looking at is reddit penny stocks found on kaggle.","text":""},{"location":"train/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Dataset</li> <li>Model Architecture</li> <li>Training</li> <li>Hyperparameter Optimization</li> </ol>"},{"location":"train/#overview","title":"Overview","text":"<p>This pipeline includes the following features: - Data preprocessing and tokenization using <code>AutoTokenizer</code> from the Hugging Face Transformers library. - Sentiment regression model built on <code>DistilBERT</code>. - Fine-tuning using <code>PyTorch Lightning</code>. - Automated hyperparameter tuning with <code>Optuna</code>. - Configurable via <code>Hydra</code> to maintain modular and scalable configurations.</p>"},{"location":"train/#dataset","title":"Dataset","text":"<p>The dataset is provided as a CSV file with the following structure:</p> Sentence Sentiment \"The stock is performing well\" positive \"The market looks bad today\" negative \"I'm neutral about this move\" neutral <ul> <li>File Location: <code>data/raw/data.csv</code></li> <li>Columns:</li> <li><code>Sentence</code>: The text input.</li> <li><code>Sentiment</code>: The sentiment label (<code>positive</code>, <code>negative</code>, <code>neutral</code>).</li> </ul>"},{"location":"train/#model-architecture","title":"Model Architecture","text":""},{"location":"train/#sentiment-regression-model","title":"Sentiment Regression Model","text":"<ul> <li>Base Model: <code>DistilBERT</code></li> <li>Tokenization: Uses <code>AutoTokenizer</code> from Hugging Face to preprocess the text.</li> <li>Regression Head: A single fully connected layer on top of the [CLS] token representation to predict a continuous sentiment score.</li> <li>Loss Function: Mean Squared Error (MSE).</li> </ul>"},{"location":"train/#training","title":"Training","text":""},{"location":"train/#key-components","title":"Key Components","text":"<ul> <li>Training Split: The dataset is split into training and validation sets based on <code>train_split</code> in the configuration.</li> <li>Optimizer: <code>AdamW</code> with configurable learning rate and L2 regularization.</li> <li>Trainer: <code>PyTorch Lightning</code> handles the training and validation loops.</li> </ul>"},{"location":"train/#training-steps","title":"Training Steps","text":"<ol> <li>Load Dataset: Read the CSV file and preprocess sentences using the tokenizer.</li> <li>Dataset Splitting: Split the dataset into training and validation sets.</li> <li>Model Training: Train the model using the configured optimizer and loss function.</li> </ol>"},{"location":"train/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<ul> <li>Framework: <code>Optuna</code> is used for hyperparameter tuning.</li> <li>Optimization Goals: Minimize the validation loss.</li> <li>Tunable Parameters:</li> <li>Learning Rate (<code>lr_min</code> to <code>lr_max</code>).</li> <li>L2 Regularization (<code>l2_min</code> to <code>l2_max</code>).</li> <li>Batch Size.</li> <li>Number of Trials: <code>n_trials</code> as specified in the configuration.</li> <li>Storage: Results are stored in <code>optuna_study.db</code> using SQLite.</li> </ul>"},{"location":"train/#best-hyperparameters-example","title":"Best Hyperparameters Example:","text":"<p>```text Best trial:   Value: 0.01234   Params:     learning_rate: 6.67e-05     l2: 0.000695     batch_size: 48</p>"}]}